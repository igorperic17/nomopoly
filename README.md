# ğŸ” nomopoly - No More Polynomial Commitments!

**nomopoly** is a neural network compiler that creates Zero Knowledge ML (ZKML) systems using **Holographic Reduced Representations (HRR)** and adversarial training. Instead of traditional polynomial commitments, nomopoly uses fixed-size holographic proofs and binary verification to create practical ZK systems for machine learning inference.

## âœ¨ Key Features

- **ğŸ§  HRR-Based Proof Generation**: Fixed-size proofs using holographic memory regardless of network complexity
- **ğŸ¯ Binary Proof Verification**: Verifier learns to distinguish real vs fake proofs  
- **ğŸ”’ Frozen Prover Architecture**: Original network + HRR wrapper (never retrained)
- **ğŸ“Š MNIST Classification**: Real-world example with 14Ã—14 MNIST digit classification
- **âš¡ Adversarial Training**: GAN-like training for robust proof systems
- **ğŸ“ˆ Comprehensive Metrics**: Binary classification accuracy, score separation, proof authenticity

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/your-username/nomopoly.git
cd nomopoly

# Create virtual environment (recommended)
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

### Run the Demo

```bash
# Run the complete HRR demo
python demo.py
```

This will:
1. Demonstrate HRR innovation and fixed-size proof generation
2. Test holographic memory components (circular convolution, binding)
3. Train the binary proof verification system on MNIST
4. Show live proof verification with real vs fake proof scores
5. Generate comprehensive training plots

## ğŸ§  Holographic Reduced Representations (HRR)

### The Scalability Problem (SOLVED!)

Traditional ZK approaches suffer from proof size explosion:
- Simple network (34K params) â†’ Large proof tensor
- Complex network (1M+ params) â†’ Huge proof tensor (OOM!)

**HRR Solution**: FIXED proof size regardless of network complexity!
- Any network â†’ Proof: `[batch, 64]` (FIXED SIZE!)

### Key HRR Principles

1. **Circular Convolution**: `bind(activation, position) = circular_conv(a, p)`
2. **Superposition Memory**: `memory = 0.95 * old_memory + new_binding`
3. **Fixed Compression**: Any activation tensor â†’ 512D representation
4. **Position Encoding**: Deterministic vectors preserve layer order information

## ğŸ—ï¸ Architecture

### Core Components

#### 1. ZKProverNet (Frozen)
- **Original MNIST Network** + **HolographicWrapper**
- Generates authentic proofs using holographic memory
- **NEVER TRAINED** - remains frozen throughout process
- Architecture: `OriginalMNISTNet` â†’ `HolographicWrapper` â†’ `(result, proof)`

#### 2. ZKVerifierNet (Binary Classifier)
- Learns to distinguish real proofs from fake proofs
- Input: `(proof, result)` â†’ Output: `verification_score` (0-1)
- Dynamic layer building for flexibility
- Target: >85% binary classification accuracy

#### 3. ZKAdversarialNet (Fake Proof Generator)
- Generates fake proofs that try to fool the verifier
- Separate classifier + dedicated fake proof generator
- Uses neural networks (not HRR) to create distinguishable fakes
- Adversarial training against verifier

### Training Process

**Binary Proof Verification Training**:

1. **Verifier Training**: Learn to output 1 for real proofs, 0 for fake proofs
2. **Adversary Training**: Generate fake proofs that fool verifier (get score close to 1)
3. **Prover**: FROZEN - only generates authentic HRR proofs

**Success Metrics**:
- Binary Classification Accuracy > 85%
- Real Proof Detection > 90%
- Fake Proof Rejection > 80%
- Score Separation > 0.3

## ğŸ“Š Usage Example

```python
from nomopoly import ZKProverNet, ZKVerifierNet, ZKAdversarialNet, ZKTrainer

# Create HRR-based networks
input_dim, output_dim, proof_dim = 196, 10, 64  # MNIST 14x14 â†’ 10 classes

prover = ZKProverNet(input_dim, output_dim, proof_dim)      # Frozen HRR prover
verifier = ZKVerifierNet(input_dim, output_dim, proof_dim)  # Binary classifier
adversary = ZKAdversarialNet(input_dim, output_dim, proof_dim)  # Fake proof generator

# Initialize binary proof verification trainer
trainer = ZKTrainer(
    inference_net=prover,    # Frozen
    verifier_net=verifier,   # Learns binary classification
    malicious_net=adversary, # Generates fake proofs
    device="mps"
)

# Train binary proof verification system
stats = trainer.train(num_epochs=50, num_samples=3000)

# Results: Binary accuracy, real detection, fake rejection, score separation
```

## ğŸ”¬ Current Implementation: MNIST Classification

The current implementation demonstrates HRR on **MNIST digit classification**:

- **Input**: 14Ã—14 MNIST images (196 dimensions)
- **Output**: 10 digit classes (0-9)
- **Proof Size**: Fixed `[batch, 64]` regardless of network complexity
- **Classification Accuracy**: ~95% on MNIST
- **Binary Verification**: 76.6% accuracy (real vs fake proof distinction)

### Recent Performance Results

```
ğŸ“ˆ FINAL PERFORMANCE:
   ğŸ¯ Binary Classification Accuracy: 76.6%
   âœ… Real Proof Detection: 100.0%
   âŒ Fake Proof Rejection: 53.1%
   ğŸ­ Malicious Success Rate: 46.9%
   ğŸ“Š Score Separation: 0.509
```

**Status**: âœ… GOOD - Decent binary classification performance with healthy score separation

## ğŸ“ˆ Benchmarking

nomopoly includes comprehensive HRR benchmarking:

```python
# Training metrics automatically tracked
stats = trainer.train(num_epochs=50)

# Key metrics:
# - binary_accuracy: Overall verifier performance
# - real_correct: Real proof detection rate  
# - fake_correct: Fake proof rejection rate
# - score_separation: Real vs fake proof score gap
# - malicious_success: Adversary fooling rate
```

**Visualization**: Training progress automatically plotted to `plots/hrr_training_progress.png`

## ğŸ—‚ï¸ Project Structure

```
nomopoly/
â”œâ”€â”€ nomopoly/                    # Main package
â”‚   â”œâ”€â”€ __init__.py             # Package initialization  
â”‚   â”œâ”€â”€ networks.py             # HRR networks (HolographicMemory, ZKProverNet, etc.)
â”‚   â”œâ”€â”€ training.py             # Binary proof verification training
â”‚   â”œâ”€â”€ utils.py                # ONNX utilities
â”‚   â””â”€â”€ benchmarks.py           # Evaluation tools
â”œâ”€â”€ demo.py                     # Complete HRR demo
â”œâ”€â”€ data/                       # MNIST dataset storage
â”œâ”€â”€ plots/                      # Training visualizations
â”œâ”€â”€ exported_models/            # ONNX model exports (when supported)
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ”® Future Roadmap

### Immediate Goals
- [ ] Push binary classification accuracy above 85%
- [ ] Optimize HRR memory size and proof dimensions
- [ ] Support larger MNIST networks (28Ã—28)
- [ ] ONNX export compatibility for HRR systems

### Long-term Vision  
- [ ] Support for complex networks (CNN, ResNet, Transformers)
- [ ] Multi-class proof verification beyond binary
- [ ] Integration with existing ZK proof systems
- [ ] Formal security analysis of HRR-based proofs
- [ ] Production deployment tools

## ğŸ’¡ Key Innovations

1. **Fixed-Size Proofs**: HRR solves the proof size explosion problem
2. **Holographic Memory**: Circular convolution for activation binding
3. **Binary Verification**: Simple but effective real vs fake classification
4. **Frozen Prover**: Original network never changes, only wrapped with HRR
5. **Hook-Free Architecture**: Direct integration without fragile forward hooks

## ğŸ¤ Contributing

We welcome contributions! Key areas:
- Improving binary classification accuracy
- Optimizing HRR parameters
- Adding support for more complex networks
- Performance optimizations

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Inspired by holographic memory and distributed representations
- Built on PyTorch and torchvision for MNIST integration
- Thanks to the zero-knowledge cryptography research community
- HRR concepts from cognitive science and neural computation