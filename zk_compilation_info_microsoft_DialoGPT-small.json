{
  "model_name": "microsoft/DialoGPT-small",
  "original_model_info": {
    "model_name": "microsoft/DialoGPT-small",
    "config": {
      "vocab_size": 50257,
      "n_positions": 1024,
      "n_embd": 768,
      "n_layer": 12,
      "n_head": 12,
      "n_inner": null,
      "activation_function": "gelu_new",
      "resid_pdrop": 0.1,
      "embd_pdrop": 0.1,
      "attn_pdrop": 0.1,
      "layer_norm_epsilon": 1e-05,
      "initializer_range": 0.02,
      "summary_type": "cls_index",
      "summary_use_proj": true,
      "summary_activation": null,
      "summary_first_dropout": 0.1,
      "summary_proj_to_labels": true,
      "scale_attn_weights": true,
      "use_cache": true,
      "scale_attn_by_inverse_layer_idx": false,
      "reorder_and_upcast_attn": false,
      "bos_token_id": 50256,
      "eos_token_id": 50256,
      "return_dict": true,
      "output_hidden_states": false,
      "torchscript": false,
      "torch_dtype": null,
      "use_bfloat16": false,
      "tf_legacy_loss": false,
      "pruned_heads": {},
      "tie_word_embeddings": true,
      "chunk_size_feed_forward": 0,
      "is_encoder_decoder": false,
      "is_decoder": false,
      "cross_attention_hidden_size": null,
      "add_cross_attention": false,
      "tie_encoder_decoder": false,
      "max_length": 20,
      "min_length": 0,
      "do_sample": false,
      "early_stopping": false,
      "num_beams": 1,
      "num_beam_groups": 1,
      "diversity_penalty": 0.0,
      "temperature": 1.0,
      "top_k": 50,
      "top_p": 1.0,
      "typical_p": 1.0,
      "repetition_penalty": 1.0,
      "length_penalty": 1.0,
      "no_repeat_ngram_size": 0,
      "encoder_no_repeat_ngram_size": 0,
      "bad_words_ids": null,
      "num_return_sequences": 1,
      "output_scores": false,
      "return_dict_in_generate": false,
      "forced_bos_token_id": null,
      "forced_eos_token_id": null,
      "remove_invalid_values": false,
      "exponential_decay_length_penalty": null,
      "suppress_tokens": null,
      "begin_suppress_tokens": null,
      "architectures": [
        "GPT2LMHeadModel"
      ],
      "finetuning_task": null,
      "id2label": {
        "0": "LABEL_0",
        "1": "LABEL_1"
      },
      "label2id": {
        "LABEL_0": 0,
        "LABEL_1": 1
      },
      "tokenizer_class": null,
      "prefix": null,
      "pad_token_id": null,
      "sep_token_id": null,
      "decoder_start_token_id": null,
      "task_specific_params": {
        "conversational": {
          "max_length": 1000
        }
      },
      "problem_type": null,
      "_name_or_path": "microsoft/DialoGPT-small",
      "transformers_version": "4.53.0",
      "model_type": "gpt2",
      "n_ctx": 1024,
      "output_attentions": false
    },
    "vocab_size": 50257,
    "hidden_size": 768,
    "num_layers": 12,
    "num_attention_heads": 12,
    "max_length": 64,
    "batch_size": 1,
    "model_type": "gpt2",
    "input_shape": [
      1,
      64
    ],
    "output_shape": [
      1,
      64,
      768
    ]
  },
  "zk_compilation": {
    "total_operations": 64,
    "proof_dimension": 4096,
    "operations_list": [
      "embedding_1",
      "embedding_2",
      "dropout_1",
      "layernorm_1",
      "dropout_2",
      "dropout_3",
      "layernorm_2",
      "dropout_4",
      "layernorm_3",
      "dropout_5",
      "dropout_6",
      "layernorm_4",
      "dropout_7",
      "layernorm_5",
      "dropout_8",
      "dropout_9",
      "layernorm_6",
      "dropout_10",
      "layernorm_7",
      "dropout_11",
      "dropout_12",
      "layernorm_8",
      "dropout_13",
      "layernorm_9",
      "dropout_14",
      "dropout_15",
      "layernorm_10",
      "dropout_16",
      "layernorm_11",
      "dropout_17",
      "dropout_18",
      "layernorm_12",
      "dropout_19",
      "layernorm_13",
      "dropout_20",
      "dropout_21",
      "layernorm_14",
      "dropout_22",
      "layernorm_15",
      "dropout_23",
      "dropout_24",
      "layernorm_16",
      "dropout_25",
      "layernorm_17",
      "dropout_26",
      "dropout_27",
      "layernorm_18",
      "dropout_28",
      "layernorm_19",
      "dropout_29",
      "dropout_30",
      "layernorm_20",
      "dropout_31",
      "layernorm_21",
      "dropout_32",
      "dropout_33",
      "layernorm_22",
      "dropout_34",
      "layernorm_23",
      "dropout_35",
      "dropout_36",
      "layernorm_24",
      "dropout_37",
      "layernorm_25"
    ],
    "operation_order": [
      "embedding_1",
      "embedding_2",
      "dropout_1",
      "layernorm_1",
      "dropout_2",
      "dropout_3",
      "layernorm_2",
      "dropout_4",
      "layernorm_3",
      "dropout_5",
      "dropout_6",
      "layernorm_4",
      "dropout_7",
      "layernorm_5",
      "dropout_8",
      "dropout_9",
      "layernorm_6",
      "dropout_10",
      "layernorm_7",
      "dropout_11",
      "dropout_12",
      "layernorm_8",
      "dropout_13",
      "layernorm_9",
      "dropout_14",
      "dropout_15",
      "layernorm_10",
      "dropout_16",
      "layernorm_11",
      "dropout_17",
      "dropout_18",
      "layernorm_12",
      "dropout_19",
      "layernorm_13",
      "dropout_20",
      "dropout_21",
      "layernorm_14",
      "dropout_22",
      "layernorm_15",
      "dropout_23",
      "dropout_24",
      "layernorm_16",
      "dropout_25",
      "layernorm_17",
      "dropout_26",
      "dropout_27",
      "layernorm_18",
      "dropout_28",
      "layernorm_19",
      "dropout_29",
      "dropout_30",
      "layernorm_20",
      "dropout_31",
      "layernorm_21",
      "dropout_32",
      "dropout_33",
      "layernorm_22",
      "dropout_34",
      "layernorm_23",
      "dropout_35",
      "dropout_36",
      "layernorm_24",
      "dropout_37",
      "layernorm_25"
    ]
  },
  "files": {
    "zk_graph_pytorch": "llama_zk_graph_microsoft_DialoGPT-small.pt",
    "zk_graph_onnx": "zk_graph_microsoft_DialoGPT-small.onnx",
    "original_onnx": "original_model_microsoft_DialoGPT-small.onnx"
  },
  "visualization": {
    "zk_graph_nodes": 8,
    "original_nodes": 2513,
    "netron_command": "netron zk_graph_microsoft_DialoGPT-small.onnx"
  }
}